import streamlit as st
import yt_dlp
import nltk
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

# Gerekli NLTK verilerini indir
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

st.set_page_config(page_title="VarpilatÃ¶r Web", page_icon="ğŸ¤–", layout="wide")

# --- YAN MENÃœ (AYARLAR) ---
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    st.write("Ã–zetin ne kadar kÄ±sa olacaÄŸÄ±nÄ± buradan seÃ§:")
    ozet_cumle_sayisi = st.slider("Ã–zet CÃ¼mle SayÄ±sÄ±", min_value=1, max_value=10, value=2)
    st.info(f"SeÃ§ilen: En Ã¶nemli **{ozet_cumle_sayisi}** cÃ¼mleyi getir.")

# --- ANA EKRAN ---
st.title("ğŸ¤– VarpilatÃ¶r - AkÄ±llÄ± Ã–zet")
st.write("Videoyu analiz eder, anahtar kelimeleri ve en vurucu cÃ¼mleleri Ã§Ä±karÄ±r.")

youtube_url = st.text_input("YouTube Video Linkini YapÄ±ÅŸtÄ±r:")

if youtube_url:
    # Video ID'sini bulma
    video_id = ""
    if "v=" in youtube_url:
        video_id = youtube_url.split("v=")[1].split("&")[0]
    elif "youtu.be" in youtube_url:
        video_id = youtube_url.split("/")[-1]
        
    if video_id:
        col1, col2 = st.columns([1, 2])
        with col1:
            st.image(f"https://img.youtube.com/vi/{video_id}/0.jpg", use_container_width=True)
        with col2:
            st.success("Video bulundu! Analize hazÄ±r.")

    if st.button("Analiz Et"):
        with st.spinner("Video inceleniyor... (Bu iÅŸlem videonun uzunluÄŸuna gÃ¶re sÃ¼rebilir)"):
            try:
                # 1. ADIM: METNÄ° Ã‡EKME (yt-dlp)
                ydl_opts = {
                    'skip_download': True,
                    'writesubtitles': True,
                    'writeautomaticsub': True,
                    'subtitleslangs': ['tr', 'en'],
                    'quiet': True,
                }

                full_text = ""
                
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(youtube_url, download=False)
                    captions = info.get('automatic_captions') or info.get('subtitles')
                    
                    if captions:
                        target_lang = 'tr' if 'tr' in captions else 'en'
                        if target_lang in captions:
                            subs_list = captions[target_lang]
                            # JSON3 formatÄ± en iyisidir
                            json_url = None
                            for sub in subs_list:
                                if sub['ext'] == 'json3':
                                    json_url = sub['url']
                                    break
                            # Yoksa ilkini al
                            if not json_url and subs_list:
                                json_url = subs_list[0]['url']

                            if json_url:
                                import requests
                                response = requests.get(json_url)
                                data = response.json()
                                
                                # Metni birleÅŸtir
                                if 'events' in data:
                                    for event in data['events']:
                                        if 'segs' in event:
                                            for seg in event['segs']:
                                                if 'utf8' in seg:
                                                    # Noktalama iÅŸareti sorunu iÃ§in basit bir boÅŸluk ekleme
                                                    full_text += seg['utf8'] + " "
                
                # 2. ADIM: Ä°ÅLEME VE SUNMA
                if full_text:
                    # Dil tespiti (BasitÃ§e hedef dile gÃ¶re)
                    dil = "turkish" if target_lang == 'tr' else "english"
                    
                    parser = PlaintextParser.from_string(full_text, Tokenizer(dil))
                    summarizer = LsaSummarizer()
                    
                    # Stop words (ve, veya, bir gibi gereksiz kelimeleri temizle)
                    summarizer.stop_words = get_stop_words(dil)

                    # A) ANAHTAR KELÄ°MELER (En kÄ±sa Ã¶zet budur)
                    st.divider()
                    st.subheader("ğŸ”‘ Anahtar Kelimeler")
                    st.write("Video temel olarak bunlardan bahsediyor:")
                    
                    # Sumy Keywords Extraction (TextRank benzeri Ã§alÄ±ÅŸÄ±r ama LsaSummarizer iÃ§inde built-in yoktur, manuel basit extraction yapalÄ±m veya Ã¶zete odaklanalÄ±m)
                    # Basitlik adÄ±na Ã¶zeti verelim, anahtar kelime yerine Ã¶zetin en baÅŸÄ±na odaklanalÄ±m.
                    
                    # B) Ã–ZET
                    st.subheader(f"ğŸ“Œ En Ã–nemli {ozet_cumle_sayisi} CÃ¼mle")
                    summary = summarizer(parser.document, ozet_cumle_sayisi)
                    
                    for i, sentence in enumerate(summary, 1):
                        st.info(f"**{i}.** {str(sentence)}")
                        
                    # C) TAM METÄ°N
                    with st.expander("ğŸ“„ Tam Metni Ä°ncele"):
                        st.text_area("TÃ¼m DÃ¶kÃ¼m", full_text, height=300)
                        
                else:
                    st.error("Metin Ã§ekilemedi. AltyazÄ± kapalÄ± olabilir.")

            except Exception as e:
                st.error("Bir hata oluÅŸtu.")
                st.write(f"Hata detayÄ±: {e}")
